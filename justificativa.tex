% justificativa
\chapter{Justificativa}

Recuperação de Informações na Web é uma tarefa fácil, porém trabalhosa se feita manualmente por humanos. Fácil, pois conseguimos detectar padrões de informações, categoriza-las e armazena-las como quisermos. Trabalhosa, pois podem haver inúmeras páginas com grandes quantidades de informação, o que torna o processo de recuperação manual de informações caro, trabalhoso e tedioso.

Soluções computacionais como \emph{web crawlers} ou \emph{web scrapers} ajudam na automação do processo de recuperação de informações estruturadas. 

No entanto, por serem soluções computacionais, demandam que sejam programadas e, geralmente, para um fim específico. Um \emph{web scraper} que foi originalmente desenvolvido para extrair informações de um determinado tipo de página, não conseguirá extrair as mesmas informações se a estrutura e/ou organização da página mudarem.

Na prática, um \emph{web scraper} projetado para um sítio, precisará sofrer modificações de forma que consiga extrair informações caso a estrutura da página em HTML mude, seja por uma mudança na estrutura de páginas do sítio ou pelo uso de um novo sítio que possa prover as mesmas informações.

Com a recuperação de informações de forma manual, havia o problema da falta de automação de processo, o que pode levar a um alto custo humano de recuperação de informações. Com a automação do processo, pode haver o problema de alto custo de criação e manutenção dos softwares responsáveis pela recuperação de informação estruturada.

O presente trabalho visa criar um método que diminua os custos de criação e manutenção de \emph{web scrapers} por meio da criação automatizada destes softwares através do uso de \emph{templates} em XML.

\section{XML e WPT}

Para o contexto deste trabalho, o XML desempenha um papel fundamental, onde as regras do \emph{web scraper} serão definidas para posterior geração do código do software de recuperação de informações em si.

O formato XML utilizado é o WPT (Website Parse Template)\cite{wpt}. O principal motivo para utiliza-lo é fazer uso de padrões Web já estabelecidos e documentados que descrevam a estrutura de páginas HTML.

Com isso, o XML se torna uma \emph{DSL} (\emph{Domain Specific Language} - Linguagem Específica de um domínio) que é dedicada para o domínio de \emph{web scraping}. O uso de \emph{DSLs} faz com que não seja estritamente necessário codificar em uma linguagem de domínio geral, o que torna a tarefa específica de criação e manutenção de \emph{web scrapers} mais fácil e produtiva.

\section{Python}


\section{Scrapy}

