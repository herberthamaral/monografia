% justificativa
\chapter{Justificativa}

Recuperação de Informações na Web é uma tarefa fácil, porém trabalhosa se feita manualmente por humanos. Fácil, pois conseguimos detectar padrões de informações, categoriza-las e armazena-las como quisermos. Trabalhosa, pois podem haver inúmeras páginas com grandes quantidades de informação, o que torna o processo de recuperação manual de informações caro, trabalhoso e tedioso.

Soluções computacionais como \emph{web crawlers} ou \emph{web scrapers} ajudam na automação do processo de recuperação de informações estruturadas. 

No entanto, por serem soluções computacionais, demandam que sejam programadas e, geralmente, para um fim específico. Um \emph{web scraper} que foi originalmente desenvolvido para extrair informações de um determinado tipo de página, não conseguirá extrair as mesmas informações se a estrutura e/ou organização da página mudarem.

Na prática, um \emph{web scraper} projetado para um sítio, precisará sofrer modificações de forma que consiga extrair informações caso a estrutura da página em HTML mude, seja por uma mudança na estrutura de páginas do sítio ou pelo uso de um novo sítio que possa prover as mesmas informações.

Com a recuperação de informações de forma manual, havia o problema da falta de automação de processo, o que pode levar a um alto custo humano de recuperação de informações. Com a automação do processo, pode haver o problema de alto custo de criação e manutenção dos softwares responsáveis pela recuperação de informação estruturada.

O presente trabalho visa criar um método que diminua os custos de criação e manutenção de \emph{web scrapers} por meio da criação automatizada destes softwares através do uso de \emph{templates} em XML.