% resultados.tex
\chapter{Resultados}

O protótipo aqui apresentado ainda encontra-se em desenvolvimento. Por ser uma contribuição ao Scrapy, um framework de código aberto, este protótipo poderá ser desenvolvido por outros membros da comunidade.

Um dos aspectos positivos da abordagem adotada é que a ferramenta desenvolvida gera código na linguagem de programação Python a partir de código em XML, permitindo que o programador faça altarações no código gerado, caso seja necessário. 

Como este trabalho não contempla, tampouco pretende contemplar, todas as facetas da criação de sistemas de recuperação de informações estruturadas na Web, a possibilidade do programador alterar o código gerado pode vir a ser bem útil em situações mais específicas. Por isso o objetivo deste trabalho é \emph{gerar código} ao invés de \emph{interpretar código} com o intuito de criar sistemas de recuperação de informações.

Pelo fato dos objetivos deste trabalho se basearem na criação de uma ferramenta que facilite a criação de sistemas de recuperção de informação na Web, duas análises podem ser feitas com os resultados: uma quantitativa e outra qualitativa. Quantitativa para mostrar as visíveis economias no processo de escrita/manutenção de código. Qualitativa para mostrar como os resultados obtidos podem levar a uma maior produtividade, menor curva de aprendizado e resultados mais rápidos. Também se faz necessária a demonstração de exemplos de sistemas de recuperação de informações reais criados a partir do uso da abordagem desenvolvida neste trabalho, bem como as dificuldades encontras e trabalhos futuros.

A seguir, duas avaliações são mostradas, quantitativas e qualitativas, dos resultados obtidos. Logo a seguir, há exemplos de spiders reais gerados a partir da ferramenta, dificuldades encontradas e trabalhos futuros.

\section{Avaliação quantitativa}

A avaliação de sucesso no cumprimento dos objetivos deste projeto pode ser meramente subjetiva, porém, há alguns critérios quantitativos ou subjetivos que podem ser discutidos, como a economia em linhas de código. No exemplo mais simples possível, há o seguinte template:

\lstset{language=XML,
basicstyle=\scriptsize,
caption={Exemplo de template em WPT},
captionpos=b
}
\begin{lstlisting}
  <ow:wpt xmlns:ow="http://www.omfica.org/schemas/ow/0.9"
            ow:host="http://example.com">
  <ow:template ow:name="Template Example" ow:url="http://www.example.com/index.php">
      <ow:block ow:tagid="ex1" name="ex1"></ow:block>
  </ow:template> 
  </ow:wpt>
\end{lstlisting}

O template anterior gera o seguinte código em Python:

\lstset{language=Python,
basicstyle=\scriptsize,
caption={\emph{Spider} gerado a partir do arquivo de entrada apresentado na listagem 5.3},
captionpos=b
}
\begin{lstlisting}
from scrapy.item import Item, Field

class TemplateExampleItem1(Item):
    ex1 = Field()

from scrapy.spider import BaseSpider
from scrapy.contrib.loader import XPathItemLoader

class TemplateExample(BaseSpider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = [
        'http://www.example.com/index.php',
    ]
    
    def parse(self, response):
        l = XPathItemLoader(item = TemplateExampleItem1(),response=response)
        l.add_xpath('bubble','id("ex1")/text()') 
        i = l.load_item()
        yield i
\end{lstlisting}

O código em Python possui 20 linhas, 16 sem as linhas em branco, ao passo que o template possui apenas 5 linhas de código. Neste exemplo, na pior das hipóteses, a economia de código gira em torno de 2/3 do original. A diferença fica mais evidente em um exemplo mais completo:

\lstset{language=XML,
basicstyle=\scriptsize,
caption={Exemplo mais completo de template em WPT},
captionpos=b
}
\begin{lstlisting}
<ow:wpt xmlns:ow="http://www.omfica.org/schemas/ow/0.9"
ow:host="http://example.com">
<ow:template ow:name="Template Example" ow:url="http://www.example.com/index.php">
  <ow:block ow:tagid="ex1" name="ex1"></ow:block>
  <ow:block ow:xpath="/html/body/p/text()" name="paragraph"></ow:block>
  
  <ow:block ow:xpath="/html/body/ul/li" name="menu">
    <ow:block ow:xpath="//a/text()" name="item" ow:type="repetable"></ow:block>
    <ow:block ow:xpath="//a[@href]" name="url" ow:type="repetable"></ow:block>
  </ow:block>
  
  <ow:block ow:xpath="/html/body/div[2]/ul/li" name="paginate">
    <ow:block ow:xpath="//a[@href]" name="url" ow:type="follow"></ow:block> <!-- follow -->
  </ow:block>
</ow:template> 
</ow:wpt>
\end{lstlisting}

\pagebreak
\lstset{language=Python,
basicstyle=\scriptsize,
caption={\emph{Spider} gerado a partir do arquivo de entrada apresentado na listagem 5.3},
captionpos=b
}
\begin{lstlisting}
from scrapy.item import Item, Field

class TemplateExampleItem1(Item):
    ex1 = Field()

class TemplateExampleItem2(Item):
    paragraph = Field()
    
class MenuItem(Item):
    item = Field()
    url = Field()
    
class Paginate(Item):
    url = Field()

from scrapy.spider import BaseSpider
from scrapy.contrib.loader import XPathItemLoader
from scrapy.selector import HtmlXPathSelector
from scrapy.http import Request

class TemplateExample(BaseSpider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = [
        'http://www.example.com/index.php',
    ]

    def parse(self, response):
        l = XPathItemLoader(item = TemplateExampleItem1(),response=response)
        l.add_xpath('bubble','id("ex1")/text()') 
        i = l.load_item()
        yield i
        
        l = XPathItemLoader(item = TemplateExampleItem2(),response=response)
        l.add_xpath('paragraph','/html/body/p/text()') 
        i = l.load_item()
        yield i
        
        hxs = HtmlXPathSelector(response)
        for item in hxs.select('/html/body/ul/li'):
            l = XPathItemLoader(item = MenuItem(), selector=item)
            l.add_xpath('item','//li/a/text()') 
            l.add_xpath('url','//li/a[@href]') 
            i = l.load_item()
            yield i
                        
        hxs = HtmlXPathSelector(response)
        for item in hxs.select('/html/body/div[2]/ul/li'):
            l = XPathItemLoader(item = Paginate(), selector=item)
            l.add_xpath('url','//li/a[@href]') 
            i = l.load_item()
            yield Request(item=i['url'],callback=self.parse)
            yield i
\end{lstlisting}

Neste segundo exemplo, o arquivo XML possui 15 linhas, ao passo que o código gerado possui cerca de 60 linhas, aproximadamente 4 vezes menos código.

\pagebreak
\section{Avaliação qualitativa}

Em uma comparação qualitativa, pode-se citar alguns critérios:

\begin{enumerate}
	\item Linguagens de propósito geral \emph{versus} linguagens de proposíto específico (ou Domain Specific Languages).
	\item Ganho de produtividade.
	\item Facilidade para o usuário final.
	\item Automação de processos.
\end{enumerate}

Linguagens de propósito geral servem para construção de mais diversos tipos de sistemas, ao passo que linguagens de propósito específico são projetadas para resolver problemas em um domínio específico. Há um \emph{tradeoff} entre as duas abordagens: enquanto as linguagens de propósito geral são mais completas, elas também são mais complexas se comparadas com as linguagens de propósito específico para a mesma categoria de problema.

Quando se deixa de usar uma linguagem de propósito geral (Python) para utilizar outra de propósito específico (XML/WPT) o ganho de produtividade estará em eliminar preocupações não inerentes ao domínio. Com menos recursos e preocupações para se trabalhar, o trabalho do usuário final é facilitado.

Por exemplo, quando se trabalha com uma linguagem de propósito geral (como Python, C/C++ ou Java), o programador dispõe de uma maior flexibilidade, pois o mesmo pode valer de estruturas de controle, repetição, variáveis, funções ou métodos, classes, objetos, interação com o sistema de arquivos e E/S, interação com outros sistemas e tratamento de exceções para desenvolver suas soluções. Além de todos os itens relacionados à linguagem de programação, o programador precisa ter domínio de técnicas de extração de informações de páginas na Web, em que, nos casos mais simples envolve o conhecimento de uma linguagem de marcação (HTML, por exemplo), bibliotecas utilizadas para \emph{parsing} e bibliotecas para comunicação com servidores HTTP (\emph{HyperText Transfer Protocol} - Protocolo de Transferência de HiperTexto). No caso de uma linguagem específica de domínio baseada em XML, o programador precisará ter as mesmas noções de extração de informações de páginas na Web, porém sem a complexidade de lidar com uma linguagem de propósito geral e todas as suas respectivas dependências (bibliotecas), limitando-se apenas a conhecer a sintaxe do XML e as regras inerentes ao WPT.

O uso de XML pode trazer várias vantagens à criação de sistemas de recuperação de informação. A principal é que qualquer linguagem de programação que dê suporte à XML pode criar templates. Como o trabalho de \emph{não} escrever código é sempre menor do que escrever \emph{algum} código, os usuários podem se beneficiar com a criação de sistemas que geram templates automaticamente. Por exemplo, um \emph{plugin} de um navegador Web pode permitir que o usuário selecione as seções de uma determinada página que gostaria que fossem recuperadas automaticamente e então gerar o template para o mesmo.

\section{Exemplos reais}

\pagebreak
\section{Dificuldades encontradas}

Apesar da abundante documentação e mesmo não possuindo uma má qualidade de código ou baixa cobertura de testes, uma das principais dificuldades foi aderir à arquitetura e estilo de codificação do Scrapy para que a padronização entre componentes fosse mantida.

O framework em si possui um tamanho considerável (759 arquivos em 13/11/2010 \footnote{Informação obtida através do comando \texttt{find | wc -l} }), portanto, leva-se um tempo para acostumar-se com a estrutura de diretórios, arquivos e padrões de nome.

Não foi possível encontrar referências acadêmicas do Scrapy, um dos motivos pelos quais somente a respectiva documentação oficial foi utilizada.

O formato WPT possui apenas informações sobre a disposição dos elementos em uma página e seus respectivos significados (ontologia). Tal fato leva ao incoveniente de haver diferentes \emph{schemas} XML em um mesmo arquivo para determinar outras informações, como nome dos variáveis do código gerado e formato de persistência dos dados obtidos.

A biblioteca padrão para testes disponíveis no Python 2.6 (utilizado neste trabalho), a \texttt{unittest}, não possui um método de comparação de strings linha-a-linha. Isso significa que, se um texto precisa ser comparado com outro em um teste unitário, os dois serão comparados em sua totalidade, não linha-a-linha. 

Esta dificuldade torna os testes de geração de código mais trabalhosos, pois é necessário observar cada caractere com cautela quando os dois textos, o esperado e o obtido, falham no teste unitário.

Outra dificuldade encontrada nos testes unitários foi o teste de comandos, devido ao tempo que os testes levam para serem executados. A suíte de testes do Scrapy levam aproximadamente 30 segundos para serem executados\footnote{Os testes são executados com o comando \texttt{bin/runtests.sh scrapy.tests} dentro do diretório raiz do código fonte do Scrapy }, o que atrasa o desenvolvimento, já que a cada modificação no código, os testes precisam ser executados.

\pagebreak
\section{Trabalhos futuros}

O atual sistema ainda possui algumas limitações, como a falta de persistência dos dados obtidos. No atual cenário, é necessário que o programador altere o código fonte gerado para que os itens gerados pelo Scrapy durante a recuperação de informações sejam persistidos em disco.

O presente trabalho conseguiu atingir o objetivo de facilitar a criação e manutenção de sistemas de recuperação estruturada na Web, porém é possível facilitar ainda mais este trabalho. Plugins para navegadores Web podem ser criados de forma que os usuários possam criar os Website Parse Templates (WPT) através de uma interface visual.

Como o significado dos dados obtidos (metadados) não fazem parte deste trabalho, a seção \texttt{<ow:ontology>} do WPT é deliberadamente ignorada. Em trabalhos futuros, é provável que a mesma seja utilizada e então o protótipo aqui criado terá de ser modificado. 

\emph{Plugins} como o Firebug \cite{firebug} para o navegador Mozilla Firefox permitem inspecionar a estrutura do conteúdo de sítios na Web. Com ele, é possível selecionar elementos, mudar estilos e conteúdo, analisar o tráfego utilizado pelo acesso ao sítio e \emph{debugar} JavaScript. Usando o Firebug como plataforma, é possível desenvolver um outro complemento que facilite a criação de templates, o que poderia facilitar ainda mais o trabalho de criação de sistemas de recuperção de informações na Web.